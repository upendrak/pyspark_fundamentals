{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/upendrak/pyspark_fundamentals/blob/main/PySpark_RDD_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "id": "up3OXE1PgUEn",
        "outputId": "4d0c1803-7543-4c13-a899-883a13594200",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.0)\n",
            "Requirement already satisfied: py4j==0.10.9.2 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializing Spark using SparkContext\n",
        "SparkContext is the entry point to any spark functionality. When we run any Spark application, a driver program starts, which has the main function and your SparkContext gets initiated here. The driver program then runs the operations inside the executors on worker nodes"
      ],
      "metadata": {
        "id": "3G8fhsF0kXqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SparkContext Example"
      ],
      "metadata": {
        "id": "mdni0T_vk6hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext(\"local\", \"Demo\")"
      ],
      "metadata": {
        "id": "A55g_zvWgVZS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sc.version\n",
        "Check the version of PySpark installed"
      ],
      "metadata": {
        "id": "XiMyeWiOkKP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc.version"
      ],
      "metadata": {
        "id": "XdK1tObeg76n",
        "outputId": "b5f3858c-1df1-43b7-e1e4-a5648d1c2ed1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3.2.0'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sc.pythonVer\n",
        "Check the version of Python installed with PySpark"
      ],
      "metadata": {
        "id": "AwdmG30rkQFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc.pythonVer"
      ],
      "metadata": {
        "id": "Kf95ieQqhW6U",
        "outputId": "1b7f7b8b-9541-4764-9a8f-7261b2285230",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3.7'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating an RDD in PySpark\n",
        "Spark revolves around the concept of a resilient distributed dataset (RDD), which is a fault-tolerant collection of elements that can be operated on in parallel.\n",
        "\n",
        "Two different methods for creating RDD in PySpark\n",
        "1.   Parallelizing an existing collection in your driver program\n",
        "2.   referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat\n",
        "\n"
      ],
      "metadata": {
        "id": "MebsE4lylF1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating RDD using `parallelize()`"
      ],
      "metadata": {
        "id": "b-mkzoomxxna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"scala\", \n",
        "   \"java\", \n",
        "   \"hadoop\", \n",
        "   \"spark\", \n",
        "   \"pandas\",\n",
        "   \"spark vs hadoop\", \n",
        "   \"pyspark\",\n",
        "   \"pyspark and spark\",\n",
        "   \"sparklyr\",\n",
        "   \"koalas\"]\n",
        "\n",
        "words_rdd = sc.parallelize(words)"
      ],
      "metadata": {
        "id": "PpLPC_PRhZmg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## type()\n",
        "Check the type of object created"
      ],
      "metadata": {
        "id": "ZPWaKnf1kDd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(words)"
      ],
      "metadata": {
        "id": "MP0c5gMfjNfq",
        "outputId": "f413d9e8-0cf1-4799-dd58-499521d00fce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying Operations on RDD\n",
        "## Transformation: Creates a new RDD. Examples: map, filter, groupby etc.,\n",
        "## Action: Perform computations and send the result back to the driver. Examples: collect, take, first etc.,"
      ],
      "metadata": {
        "id": "qP3wCnfCl1cv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Common Actions\n",
        "Below are some of the common Action used in Spark. Refer to the [RDD API docs](https://https://https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html#pyspark.RDD) for more Actions"
      ],
      "metadata": {
        "id": "wTFDTGXhstSs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### collect()\n",
        "All the elements in the RDD are returned"
      ],
      "metadata": {
        "id": "StrrLSSckAdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_rdd.collect()"
      ],
      "metadata": {
        "id": "8L46ocMdiHsH",
        "outputId": "da3f5413-c1cf-41d1-8237-91a56bc88f52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['scala',\n",
              " 'java',\n",
              " 'hadoop',\n",
              " 'spark',\n",
              " 'pandas',\n",
              " 'spark vs hadoop',\n",
              " 'pyspark',\n",
              " 'pyspark and spark',\n",
              " 'sparklyr',\n",
              " 'koalas']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### first()\n",
        "Returns the first element in an RDD"
      ],
      "metadata": {
        "id": "tBkwB9xTqRF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_rdd.first()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YtPjDU-OqWwf",
        "outputId": "3dd68c9f-ac3a-4db4-ef6e-052393e60745"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'scala'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### take(n)\n",
        "Returns the nth element in an RDD"
      ],
      "metadata": {
        "id": "M20UNYGAqZIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_rdd.take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbab4Z1TqfXz",
        "outputId": "c3bf4f17-1bec-43fd-e62b-cce378cd92a1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['scala', 'java', 'hadoop']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### count()\n",
        "Counts the number of elements in an RDD"
      ],
      "metadata": {
        "id": "M7S7fD7blqAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_rdd.count()"
      ],
      "metadata": {
        "id": "PuD2vxpLhmYf",
        "outputId": "e510902e-658e-4783-d538-382c54f168d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### reduce()\n",
        "Combine all elements to a single result of the same type"
      ],
      "metadata": {
        "id": "wIxnOXbwuuIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nums = sc.parallelize([1,2,4,5])\n",
        "nums.reduce(lambda x, y: x + y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0R1-3UULu2wc",
        "outputId": "898d63f6-71cd-4bfd-bd4b-f5d51318e9d2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Common Transformations\n",
        "Below are some of the common Transformations used in Spark. Refer to the [RDD API docs](https://https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html#pyspark.RDD) for more Transformations"
      ],
      "metadata": {
        "id": "tQLnv8TPs4AF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### map(f)\n",
        "A new RDD is returned by applying a function to each element in an RDD"
      ],
      "metadata": {
        "id": "xex0sGwUpjRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_map = words_rdd.map(lambda x: (x,1))\n",
        "for w in words_map.collect(): print(w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQo0BFEppuHp",
        "outputId": "5edd05cd-0701-4af5-c3e9-8b88a3f3aefe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('scala', 1)\n",
            "('java', 1)\n",
            "('hadoop', 1)\n",
            "('spark', 1)\n",
            "('pandas', 1)\n",
            "('spark vs hadoop', 1)\n",
            "('pyspark', 1)\n",
            "('pyspark and spark', 1)\n",
            "('sparklyr', 1)\n",
            "('koalas', 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### flatMap(f)\n",
        "Splits the lines of baseRDD into words"
      ],
      "metadata": {
        "id": "xXoJ0vePvm4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test = [\"Pyspark is awesome\", \"Mindful feast\", \"world hunger\"]\n",
        "test_rdd = sc.parallelize(test)\n",
        "\n",
        "test_rdd.collect()\n",
        "x = test_rdd.flatMap(lambda x: x.split(' '))\n",
        "x.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EeUk4jUvmV3",
        "outputId": "3f6602fc-bcfb-42c4-b86b-f9fd10ad4c9c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Pyspark', 'is', 'awesome', 'Mindful', 'feast', 'world', 'hunger']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### filter(f)\n",
        "A new RDD is returned containing the elements, which satisfies the function inside the filter. In the following example, we filter out the strings containing ''spark\"."
      ],
      "metadata": {
        "id": "78WEE1mgjriA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res = words_rdd.filter(lambda x: \"spark\" in x)\n",
        "for w in res.collect(): print(w)"
      ],
      "metadata": {
        "id": "you9gBWNiOtW",
        "outputId": "c2981b41-2cac-4539-c20a-62110b273ca4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spark\n",
            "spark vs hadoop\n",
            "pyspark\n",
            "pyspark and spark\n",
            "sparklyr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating RDD from external datasets using `textFile()`\n"
      ],
      "metadata": {
        "id": "2h5OPkx8tRhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = sc.textFile(\"pyspark_documentation.txt\")\n",
        "type(file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVS0lVV0zAsN",
        "outputId": "b55d0462-8c8e-4197-8d8c-49ebdd924b31"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.rdd.RDD"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the lines into words and print them all\n",
        "words = file.flatMap(lambda x: x.split())\n",
        "for word in words.collect(): print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zM9XEZSmzErg",
        "outputId": "5aecec34-cb92-41f4-a9a7-839ba4f75714"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PySpark\n",
            "is\n",
            "an\n",
            "interface\n",
            "for\n",
            "Apache\n",
            "Spark\n",
            "in\n",
            "Python.\n",
            "It\n",
            "not\n",
            "only\n",
            "allows\n",
            "you\n",
            "to\n",
            "write\n",
            "Spark\n",
            "applications\n",
            "using\n",
            "Python\n",
            "APIs,\n",
            "but\n",
            "also\n",
            "provides\n",
            "the\n",
            "PySpark\n",
            "shell\n",
            "for\n",
            "interactively\n",
            "analyzing\n",
            "your\n",
            "data\n",
            "in\n",
            "a\n",
            "distributed\n",
            "environment.\n",
            "PySpark\n",
            "supports\n",
            "most\n",
            "of\n",
            "Spark's\n",
            "features\n",
            "such\n",
            "as\n",
            "Spark\n",
            "SQL,\n",
            "DataFrame,\n",
            "Streaming,\n",
            "MLlib\n",
            "(Machine\n",
            "Learning)\n",
            "and\n",
            "Spark\n",
            "Core\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the total number of words in the file\n",
        "words.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOV6QOofzTsc",
        "outputId": "0a6806b0-878f-4598-d78c-ade2a5404731"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the nunber of instance of the word \"spark\" in the file\n",
        "res = words.filter(lambda x: 'Spark' in x)\n",
        "res.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwWLh_bHzdUV",
        "outputId": "5970ad99-d33b-46ef-891e-d1ff94d4e16b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the total number of characters in the file\n",
        "lines = file.map(lambda x: len(x))\n",
        "lines.reduce(lambda x, y: x + y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pr9ZPkPzhaT",
        "outputId": "fd26f1cd-fb61-4fab-e208-81b34af92fa3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "345"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "PySpark_RDD_Basics",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}